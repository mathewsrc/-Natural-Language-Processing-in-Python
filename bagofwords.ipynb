{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9mQRLT06TMaOfRgugpFHc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathewsrc/Natural-Language-Processing-in-Python/blob/master/bagofwords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of Words\n",
        "\n",
        "* Used to finding topics in a text\n",
        "* The more frequent a word, the more import it might be"
      ],
      "metadata": {
        "id": "W2_3SMnMQAtw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmconMbwNNVp",
        "outputId": "8a8da12a-d4cd-45b7-8831-9551635c4514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article = \"\"\"'''Debugging''' is the process of finding and resolving of defects that prevent correct operation of computer software or a system.  \n",
        "\n",
        "Numerous books have been written about debugging (see below: #Further reading|Further reading), as it involves numerous aspects, including interactive debugging, control flow, integration testing, Logfile|log files, monitoring (Application monitoring|application, System Monitoring|system), memory dumps, Profiling (computer programming)|profiling, Statistical Process Control, and special design tactics to improve detection while simplifying changes.\n",
        "\n",
        "Origin\n",
        "A computer log entry from the Mark&nbsp;II, with a moth taped to the page\n",
        "\n",
        "The terms \"bug\" and \"debugging\" are popularly attributed to Admiral Grace Hopper in the 1940s.[http://foldoc.org/Grace+Hopper Grace Hopper]  from FOLDOC While she was working on a Harvard Mark II|Mark II Computer at Harvard University, her associates discovered a moth stuck in a relay and thereby impeding operation, whereupon she remarked that they were \"debugging\" the system. However the term \"bug\" in the meaning of technical error dates back at least to 1878 and Thomas Edison (see software bug for a full discussion), and \"debugging\" seems to have been used as a term in aeronautics before entering the world of computers. Indeed, in an interview Grace Hopper remarked that she was not coining the term{{Citation needed|date=July 2015}}. The moth fit the already existing terminology, so it was saved.  A letter from J. Robert Oppenheimer (director of the WWII atomic bomb \"Manhattan\" project at Los Alamos, NM) used the term in a letter to Dr. Ernest Lawrence at UC Berkeley, dated October 27, 1944,http://bancroft.berkeley.edu/Exhibits/physics/images/bigscience25.jpg regarding the recruitment of additional technical staff.\n",
        "\n",
        "The Oxford English Dictionary entry for \"debug\" quotes the term \"debugging\" used in reference to airplane engine testing in a 1945 article in the Journal of the Royal Aeronautical Society. An article in \"Airforce\" (June 1945 p.&nbsp;50) also refers to debugging, this time of aircraft cameras.  Hopper's computer bug|bug was found on September 9, 1947. The term was not adopted by computer programmers until the early 1950s.\n",
        "The seminal article by GillS. Gill, [http://www.jstor.org/stable/98663 The Diagnosis of Mistakes in Programmes on the EDSAC], Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences, Vol. 206, No. 1087 (May 22, 1951), pp. 538-554 in 1951 is the earliest in-depth discussion of programming errors, but it does not use the term \"bug\" or \"debugging\".\n",
        "In the Association for Computing Machinery|ACM's digital library, the term \"debugging\" is first used in three papers from 1952 ACM National Meetings.Robert V. D. Campbell, [http://portal.acm.org/citation.cfm?id=609784.609786 Evolution of automatic computation], Proceedings of the 1952 ACM national meeting (Pittsburgh), p 29-32, 1952.Alex Orden, [http://portal.acm.org/citation.cfm?id=609784.609793 Solution of systems of linear inequalities on a digital computer], Proceedings of the 1952 ACM national meeting (Pittsburgh), p. 91-95, 1952.Howard B. Demuth, John B. Jackson, Edmund Klein, N. Metropolis, Walter Orvedahl, James H. Richardson, [http://portal.acm.org/citation.cfm?id=800259.808982 MANIAC], Proceedings of the 1952 ACM national meeting (Toronto), p. 13-16 Two of the three use the term in quotation marks.\n",
        "By 1963 \"debugging\" was a common enough term to be mentioned in passing without explanation on page 1 of the Compatible Time-Sharing System|CTSS manual.[http://www.bitsavers.org/pdf/mit/ctss/CTSS_ProgrammersGuide.pdf The Compatible Time-Sharing System], M.I.T. Press, 1963\n",
        "\n",
        "Kidwell's article ''Stalking the Elusive Computer Bug''Peggy Aldrich Kidwell, [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?tp=&arnumber=728224&isnumber=15706 Stalking the Elusive Computer Bug], IEEE Annals of the History of Computing, 1998. discusses the etymology of \"bug\" and \"debug\" in greater detail.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gEn4Y9YiTIgp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Counter\n",
        "from collections import Counter\n",
        "\n",
        "# Tokenize the article: tokens\n",
        "tokens = word_tokenize(article)\n",
        "\n",
        "# Convert the tokens into lowercase: lower_tokens\n",
        "lower_tokens = [t.lower() for t in tokens]\n",
        "\n",
        "# Create a Counter with the lowercase tokens: bow_simple\n",
        "bow_simple = Counter(lower_tokens)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow_simple.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COiij-UuTHPS",
        "outputId": "2bf389a9-accd-4837-f215-e84e963f664e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(',', 53), ('the', 40), ('of', 24), ('.', 22), (\"''\", 18), ('in', 17), ('``', 16), ('a', 14), ('debugging', 10), ('(', 10)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why preprocess?\n",
        "\n",
        "Helps make for better input data when performing machine learning or other statistical\n",
        "methods\n",
        "\n",
        "Examples:\n",
        "* Tokenization to create a bag of words\n",
        "\n",
        "* Lowercasing words\n",
        "\n",
        "* Lemmatization/Stemming - shorten words to their root stems\n",
        "\n",
        "* Removing stop words, punctuation, or unwanted tokens\n",
        "\n",
        "* Good to experiment with different approaches\n"
      ],
      "metadata": {
        "id": "xjreK4jzWqcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# Create a english stop words list\n",
        "english_stops = stopwords.words('english')\n",
        "\n",
        "# Retain alphabetic words: alpha_only\n",
        "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
        "\n",
        "# Remove all stop words: no_stops\n",
        "no_stops = [t for t in alpha_only if t not in english_stops]\n",
        "\n",
        "# Instantiate the WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize all tokens into a new list: lemmatized\n",
        "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
        "\n",
        "# Create the bag-of-words: bow\n",
        "bow = Counter(lemmatized)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2_5F9BUTbFJ",
        "outputId": "9d515ec8-8d19-4eee-84b9-4d4871dd3f1a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('term', 11), ('computer', 10), ('debugging', 10), ('http', 8), ('bug', 7), ('system', 5), ('hopper', 4), ('used', 4), ('article', 4), ('proceeding', 4)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gensim\n",
        "\n",
        "A natural language library"
      ],
      "metadata": {
        "id": "L-R6xjByagbr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A corpus is a set of texts used to help perform natural language tasks"
      ],
      "metadata": {
        "id": "TCWLJwtpanFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora.dictionary import Dictionary"
      ],
      "metadata": {
        "id": "CZUj5Oc9alK7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a list of 5 topics\n",
        "topics = [\"politics\", \"sports\", \"technology\", \"entertainment\", \"science\"]\n",
        "\n",
        "# create a list of 10 articles on those topics\n",
        "articles = [\n",
        "    \"How the COVID-19 pandemic is affecting the upcoming election\",\n",
        "    \"Top 10 moments from the 2020 Summer Olympics\",\n",
        "    \"The rise of AI and the future of work\",\n",
        "    \"10 must-watch movies of the year\",\n",
        "    \"Scientists discover new exoplanet with potential for life\",\n",
        "    \"The latest developments in the Afghanistan conflict\",\n",
        "    \"Why NBA players are speaking out on social justice issues\",\n",
        "    \"The impact of social media on mental health\",\n",
        "    \"The most anticipated video games of the year\",\n",
        "    \"Breakthroughs in gene editing technology\"\n",
        "]\n",
        "\n",
        "# print the articles variable to see the list of articles\n",
        "print(articles)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvrLdIEboMU3",
        "outputId": "5375165d-1655-4a5d-8165-e0f86506fc04"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['How the COVID-19 pandemic is affecting the upcoming election', 'Top 10 moments from the 2020 Summer Olympics', 'The rise of AI and the future of work', '10 must-watch movies of the year', 'Scientists discover new exoplanet with potential for life', 'The latest developments in the Afghanistan conflict', 'Why NBA players are speaking out on social justice issues', 'The impact of social media on mental health', 'The most anticipated video games of the year', 'Breakthroughs in gene editing technology']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize articles\n",
        "\n",
        "articles_tokens = [word_tokenize(t) for t in articles]\n",
        "\n",
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(articles_tokens)\n",
        "\n",
        "# Select the id for \"future\": future_id\n",
        "future_id = dictionary.token2id.get(\"future\")\n",
        "\n",
        "# Use future_id with the dictionary to print the word\n",
        "print(dictionary.get(future_id))\n",
        "\n",
        "# Create a MmCorpus: corpus\n",
        "corpus = [dictionary.doc2bow(article) for article in articles_tokens]\n",
        "\n",
        "# Print the first 10 word ids with their frequency counts from the fifth document\n",
        "print(corpus[4][:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Hp2C1C7bIiL",
        "outputId": "6f88b38c-5e18-4358-9cb5-487d6693f295"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "future\n",
            "[(25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dictionary create an mapping with an id for each token\n",
        "\n",
        "The corpus is a list of list where each list represents one document (doc). Each document is a series of tuples, the first item represents the tokenId from the dictionary and the second item the token frequency in the document"
      ],
      "metadata": {
        "id": "qZs-EFdhuCtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import itertools\n",
        "\n",
        "# Save the fifth document: doc\n",
        "doc = corpus[4]\n",
        "\n",
        "# Sort the doc for frequency: bow_doc\n",
        "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 words of the document alongside the count\n",
        "for word_id, word_count in bow_doc[:5]:\n",
        "    print(dictionary.get(word_id), word_count)\n",
        "    \n",
        "# Create the defaultdict: total_word_count\n",
        "total_word_count = defaultdict(int)\n",
        "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
        "        total_word_count[word_id] += word_count\n",
        "\n",
        "print()\n",
        "\n",
        "# Create a sorted list from the defaultdict: sorted_word_count\n",
        "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
        "\n",
        "# Print the top 5 words across all documents alongside the count\n",
        "for word_id, word_count in sorted_word_count[:5]:\n",
        "    print(dictionary.get(word_id), word_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSW_Zs3Sqmk3",
        "outputId": "029e6531-c9d3-4da1-d532-6ea865f88c9f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scientists 1\n",
            "discover 1\n",
            "exoplanet 1\n",
            "for 1\n",
            "life 1\n",
            "\n",
            "the 7\n",
            "of 5\n",
            "The 4\n",
            "10 2\n",
            "year 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF\n",
        "\n",
        "* Term Frequency - inverse document frequency\n",
        "* Determine the most important words in each document\n",
        "* Each corpus may have shared words, these words should be down-weighted in importance\n",
        "* Document specific frequent words are weighted high \n",
        "\n",
        "Term frequency = percentage share of the word compared to all tokens in the document \n",
        "\n",
        "Inverse document frequency = logarithm of the total number of documents in a corpora divided by the number of documents containing the term"
      ],
      "metadata": {
        "id": "IIFOhMmBsH7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.tfidfmodel import TfidfModel\n",
        "\n",
        "# Create a new TfidfModel using the corpus: tfidf\n",
        "tfidf = TfidfModel(corpus)\n",
        "\n",
        "# Calculate the tfidf weights of doc: tfidf_weights\n",
        "tfidf_weights = tfidf[doc]\n",
        "\n",
        "# Print the first five weights\n",
        "print(tfidf_weights[:5])\n",
        "print()\n",
        "\n",
        "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
        "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 weighted words\n",
        "for term_id, weight in sorted_tfidf_weights[:5]:\n",
        "    print(dictionary.get(term_id), weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euKcQgbtu59l",
        "outputId": "ee902981-0069-4126-d591-058441e1c4d3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(25, 0.3535533905932738), (26, 0.3535533905932738), (27, 0.3535533905932738), (28, 0.3535533905932738), (29, 0.3535533905932738)]\n",
            "\n",
            "Scientists 0.3535533905932738\n",
            "discover 0.3535533905932738\n",
            "exoplanet 0.3535533905932738\n",
            "for 0.3535533905932738\n",
            "life 0.3535533905932738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h_pBZ6YyxImo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}